import pandas as pd
import numpy as np
import pickle
import sklearn.ensemble as ske
from sklearn import model_selection, tree, linear_model
from sklearn.feature_selection import SelectFromModel
import joblib
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn import tree

data = pd.read_csv('data.csv', sep='|')
X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values
y = data['legitimate'].values

fsel = ske.ExtraTreesClassifier().fit(X, y)
model = SelectFromModel(fsel, prefit=True)
X_new = model.transform(X)  # now features are only 9 :)
nb_features = X_new.shape[1]  # will save value 13 as shape is (138047, 13) :}

X_train, X_test, y_train, y_test = model_selection.train_test_split(
    X_new, y, test_size=0.2)
features = []

print('%i features identified as important:' %
      nb_features)  # as mentioned above

indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. feature %s (%f)" % (
        f + 1, data.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))

print("\nNow testing Gradient Boosting Algorithm")
clf = ske.GradientBoostingClassifier(n_estimators=50)
clf.fit(X_train, y_train)  
# fit may be called as 'trained'
score = clf.score(X_test, y_test)
print("%s : %f %%" % ("Gradient Boosting", score*100))