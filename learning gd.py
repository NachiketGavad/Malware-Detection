import pandas as pd
import numpy as np
import pickle
import sklearn.ensemble as ske
from sklearn import model_selection, tree, linear_model
from sklearn.feature_selection import SelectFromModel
import joblib
from sklearn.metrics import confusion_matrix
from sklearn import tree

data = pd.read_csv('../data.csv', sep='|')
X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values
y = data['legitimate'].values

feat_select = ske.ExtraTreesClassifier().fit(X, y)
model = SelectFromModel(feat_select, prefit=True)
X_new = model.transform(X)  
nb_features = X_new.shape[1]  

X_train, X_test, y_train, y_test = model_selection.train_test_split(X_new, y, test_size=0.2)

features = []
for f in sorted(np.argsort(feat_select.feature_importances_)[::-1][:nb_features]):
    features.append(data.columns[2+f])

# algorithms = {"RandomForest": ske.RandomForestClassifier(n_estimators=50),
#               "GradientBoosting": ske.GradientBoostingClassifier(n_estimators=50),
#               "AdaBoost": ske.AdaBoostClassifier(n_estimators=100)
#               }

print("\nNow testing Gradient Boosting Algorithm")
clf = ske.GradientBoostingClassifier(n_estimators=50)
clf.fit(X_train, y_train)  
# fit may be called as 'trained'
score = clf.score(X_test, y_test)
print("%s : %f %%" % ("Gradient Boosting", score*100))

joblib.dump(clf, 'classifier_gd.pkl')
open('features_gd.pkl', 'wb').write(pickle.dumps(features))
print('save successful !!')